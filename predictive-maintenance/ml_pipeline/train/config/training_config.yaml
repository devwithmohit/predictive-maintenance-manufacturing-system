# Training Configuration

# MLflow
mlflow:
  tracking_uri: "http://localhost:5000"
  experiment_name: "predictive_maintenance_rul"
  artifact_location: "./mlruns"
  auto_log: true

# Data
data:
  feature_store_config: "../../feature_store/config/feature_store_config.yaml"
  sequence_length: 50
  stride: 1
  batch_size: 32
  validation_split: 0.15
  test_split: 0.15

  # Feature selection
  exclude_features:
    - "time"
    - "timestamp"
    - "equipment_id"
    - "cycle"
    - "feature_version"
    - "created_at"

# LSTM Model Configuration
lstm:
  model_name: "lstm_rul_predictor"

  architecture:
    # Input shape: (sequence_length, n_features)
    lstm_layers:
      - units: 128
        return_sequences: true
        dropout: 0.2
        recurrent_dropout: 0.2
      - units: 64
        return_sequences: true
        dropout: 0.2
        recurrent_dropout: 0.2
      - units: 32
        return_sequences: false
        dropout: 0.2

    # Attention mechanism
    attention:
      enabled: true
      units: 64

    # Dense layers
    dense_layers:
      - units: 64
        activation: "relu"
        dropout: 0.3
      - units: 32
        activation: "relu"
        dropout: 0.2
      - units: 16
        activation: "relu"

    # Output layer
    output:
      units: 1
      activation: "linear"

  # Training parameters
  training:
    optimizer: "adam"
    learning_rate: 0.001
    loss: "mse"
    metrics: ["mae", "mse"]

    # Learning rate schedule
    lr_schedule:
      enabled: true
      type: "reduce_on_plateau"
      factor: 0.5
      patience: 10
      min_lr: 0.00001

    # Early stopping
    early_stopping:
      enabled: true
      monitor: "val_loss"
      patience: 20
      restore_best_weights: true
      min_delta: 0.001

    epochs: 200
    batch_size: 32
    validation_split: 0.15

  # Hyperparameter tuning
  hyperparameter_tuning:
    enabled: false
    method: "random_search"  # grid_search or random_search
    n_trials: 20

    search_space:
      lstm_units_1: [64, 128, 256]
      lstm_units_2: [32, 64, 128]
      dropout_rate: [0.1, 0.2, 0.3, 0.4]
      learning_rate: [0.0001, 0.001, 0.01]
      batch_size: [16, 32, 64]

# Random Forest Configuration
random_forest:
  model_name: "rf_health_classifier"

  # Target: health_status_code (0: healthy, 1: warning, 2: critical, 3: imminent_failure)
  target_column: "health_status_code"

  # Model parameters
  model_params:
    n_estimators: 200
    max_depth: 20
    min_samples_split: 5
    min_samples_leaf: 2
    max_features: "sqrt"
    bootstrap: true
    random_state: 42
    n_jobs: -1
    class_weight: "balanced"

  # Training
  training:
    test_size: 0.15
    validation_size: 0.15
    stratify: true
    random_state: 42

  # Hyperparameter tuning
  hyperparameter_tuning:
    enabled: false
    method: "random_search"
    n_trials: 50
    cv_folds: 5

    search_space:
      n_estimators: [100, 200, 300, 500]
      max_depth: [10, 20, 30, 40, null]
      min_samples_split: [2, 5, 10]
      min_samples_leaf: [1, 2, 4]
      max_features: ["sqrt", "log2", null]
      bootstrap: [true, false]

# Cross-Validation
cross_validation:
  enabled: true
  method: "time_series"  # time_series or equipment_based
  n_splits: 5

  # Time-series CV
  time_series:
    gap: 10  # Gap between train and test (cycles)

  # Equipment-based CV
  equipment_based:
    test_equipment_ratio: 0.2

# Model Saving
model_saving:
  save_best_only: true
  save_format: "keras"  # For LSTM
  checkpoint_dir: "./checkpoints"

  # Model versioning
  versioning:
    enabled: true
    registry_name: "production_models"

# Evaluation Metrics
evaluation:
  regression_metrics:
    - "rmse"
    - "mae"
    - "r2"
    - "mape"

  classification_metrics:
    - "accuracy"
    - "precision"
    - "recall"
    - "f1"
    - "confusion_matrix"
    - "roc_auc"

  # Custom thresholds
  early_warning_threshold: 50  # RUL cycles
  critical_threshold: 30  # RUL cycles

# Logging
logging:
  level: "INFO"
  log_file: "./logs/training.log"
  console_output: true
